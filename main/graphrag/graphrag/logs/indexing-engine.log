16:36:42,919 graphrag.cli.index INFO Logging enabled at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs/indexing-engine.log
16:36:42,924 graphrag.cli.index INFO Starting pipeline run for: 20241125-163642, dry_run=False
16:36:42,925 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "qwen-max",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:13000/v1",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag",
    "reporting": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-v1",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
16:36:42,927 graphrag.index.create_pipeline_config INFO skipping workflows 
16:36:42,928 graphrag.index.run.run INFO Running pipeline
16:36:42,928 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output
16:36:42,929 graphrag.index.input.load_input INFO loading input from root_dir=input
16:36:42,929 graphrag.index.input.load_input INFO using file storage for input
16:36:42,931 graphrag.index.storage.file_pipeline_storage INFO search /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/input for files matching .*\.txt$
16:36:42,931 graphrag.index.input.text INFO found text files from input, found [('6.txt', {}), ('3.txt', {}), ('1.txt', {}), ('2.txt', {}), ('5.txt', {}), ('4.txt', {})]
16:36:42,946 graphrag.index.input.text INFO Found 6 files, loading 6
16:36:42,948 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_covariates', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
16:36:42,949 graphrag.index.run.run INFO Final # of rows loaded: 6
16:36:43,252 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
16:36:43,259 datashaper.workflow.workflow INFO executing verb create_base_text_units
16:36:44,68 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
16:36:44,77 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:36:44,92 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
16:36:44,100 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
16:36:44,146 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for qwen-max: TPM=0, RPM=0
16:36:44,146 graphrag.index.llm.load_llm INFO create concurrency limiter for qwen-max: 25
16:36:44,463 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:36:44,470 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:36:44,471 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:36:44,473 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:36:44,474 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:36:44,475 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:36:51,526 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:36:51,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.281874503940344. input_tokens=1922, output_tokens=269
16:36:54,205 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:36:54,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.002252673730254. input_tokens=1993, output_tokens=322
16:37:03,827 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:03,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.288765860721469. input_tokens=34, output_tokens=269
16:37:05,803 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:05,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.59458975493908. input_tokens=3088, output_tokens=661
16:37:10,790 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:10,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.581602221354842. input_tokens=34, output_tokens=440
16:37:11,243 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:11,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.047771997749805. input_tokens=2215, output_tokens=786
16:37:14,175 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:14,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.93349959515035. input_tokens=2135, output_tokens=1007
16:37:19,922 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:19,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.68261928483844. input_tokens=3022, output_tokens=988
16:37:22,714 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:22,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.44318631850183. input_tokens=3089, output_tokens=1140
16:37:22,738 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:22,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.553407307714224. input_tokens=34, output_tokens=193
16:37:22,790 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:22,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.541948383674026. input_tokens=34, output_tokens=245
16:37:26,483 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:26,494 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.31779204122722. input_tokens=3089, output_tokens=1158
16:37:27,775 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:27,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.50882068276405. input_tokens=3088, output_tokens=1321
16:37:29,998 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:30,0 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.805116679519415. input_tokens=3088, output_tokens=1060
16:37:30,108 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:30,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.89356603100896. input_tokens=3089, output_tokens=1411
16:37:31,775 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:31,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.59382828511298. input_tokens=3088, output_tokens=1304
16:37:32,11 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:32,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.79073381610215. input_tokens=2643, output_tokens=1092
16:37:32,238 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:32,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 46.12112367153168. input_tokens=3088, output_tokens=1446
16:37:32,587 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:32,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.78269158117473. input_tokens=34, output_tokens=698
16:37:33,244 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:33,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.057710729539394. input_tokens=3087, output_tokens=912
16:37:33,769 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:33,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.59413745813072. input_tokens=3088, output_tokens=1181
16:37:33,791 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:33,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.564736288040876. input_tokens=3088, output_tokens=1079
16:37:34,52 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:34,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.77680352702737. input_tokens=2202, output_tokens=1111
16:37:37,643 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:37,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.713895119726658. input_tokens=34, output_tokens=489
16:37:39,262 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:39,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.04832496866584. input_tokens=3088, output_tokens=1279
16:37:46,274 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:46,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.497640376910567. input_tokens=34, output_tokens=567
16:37:47,403 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:47,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 61.609392918646336. input_tokens=3087, output_tokens=1606
16:37:48,946 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:48,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.78137471713126. input_tokens=3088, output_tokens=1710
16:37:52,826 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:52,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.767972568050027. input_tokens=34, output_tokens=512
16:37:53,629 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:53,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.379052482545376. input_tokens=34, output_tokens=515
16:37:54,73 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:54,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.29288217611611. input_tokens=34, output_tokens=556
16:37:55,153 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:55,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.358197743073106. input_tokens=34, output_tokens=697
16:37:55,517 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:37:55,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.398344015702605. input_tokens=34, output_tokens=377
16:38:00,300 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:00,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 73.91922276467085. input_tokens=3088, output_tokens=1843
16:38:00,392 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:00,396 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.615014923736453. input_tokens=34, output_tokens=687
16:38:00,552 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:00,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.284545712172985. input_tokens=34, output_tokens=513
16:38:00,965 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:00,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.949412247166038. input_tokens=34, output_tokens=574
16:38:02,164 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:02,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.156843315809965. input_tokens=34, output_tokens=888
16:38:02,780 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:02,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.06190148368478. input_tokens=34, output_tokens=746
16:38:05,807 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:05,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.397951554507017. input_tokens=34, output_tokens=434
16:38:07,713 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:07,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.216732416301966. input_tokens=34, output_tokens=1101
16:38:08,793 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:08,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.547137623652816. input_tokens=34, output_tokens=918
16:38:10,804 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:10,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 21.853290485218167. input_tokens=34, output_tokens=448
16:38:13,440 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:13,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.135896319523454. input_tokens=34, output_tokens=328
16:38:31,711 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:31,715 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 107.47253615409136. input_tokens=3088, output_tokens=2660
16:38:51,810 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:51,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.095301784574986. input_tokens=34, output_tokens=459
16:38:52,132 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:38:52,135 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:38:52,136 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:38:52,137 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:38:52,138 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:38:52,139 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:38:52,139 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:38:52,141 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:38:52,142 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:38:52,143 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:38:54,383 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:54,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.488803770393133. input_tokens=349, output_tokens=73
16:38:54,537 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:54,541 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.550841799005866. input_tokens=335, output_tokens=91
16:38:54,861 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:54,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.858074625954032. input_tokens=317, output_tokens=68
16:38:55,272 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:55,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.365764459595084. input_tokens=338, output_tokens=90
16:38:55,344 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:55,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.43920467607677. input_tokens=330, output_tokens=57
16:38:55,492 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:55,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5299529787153006. input_tokens=374, output_tokens=113
16:38:56,110 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:56,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.132860779762268. input_tokens=349, output_tokens=128
16:38:56,328 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:56,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.3974764961749315. input_tokens=362, output_tokens=111
16:38:56,376 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:56,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.456052843481302. input_tokens=376, output_tokens=116
16:38:56,585 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:56,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.670580927282572. input_tokens=423, output_tokens=160
16:38:57,800 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:57,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.833367113023996. input_tokens=404, output_tokens=160
16:38:57,814 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:57,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.831284133717418. input_tokens=435, output_tokens=182
16:38:57,882 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:57,885 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.912529416382313. input_tokens=349, output_tokens=173
16:38:57,941 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:57,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 4.143684396520257. input_tokens=355, output_tokens=121
16:38:57,972 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:57,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 4.827191524207592. input_tokens=452, output_tokens=151
16:38:58,132 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:58,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.789138777181506. input_tokens=341, output_tokens=79
16:38:58,544 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:58,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.1536778546869755. input_tokens=396, output_tokens=166
16:38:58,565 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:58,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.6191450618207455. input_tokens=417, output_tokens=163
16:38:58,692 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:58,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.738307556137443. input_tokens=480, output_tokens=180
16:38:58,989 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:58,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.97947066091001. input_tokens=523, output_tokens=220
16:38:59,287 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:59,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 5.378070952370763. input_tokens=319, output_tokens=126
16:38:59,501 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:59,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.227884812280536. input_tokens=404, output_tokens=145
16:38:59,817 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:59,822 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.443256001919508. input_tokens=334, output_tokens=105
16:38:59,921 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:38:59,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.382330767810345. input_tokens=381, output_tokens=144
16:39:00,164 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:00,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.6719639878720045. input_tokens=404, output_tokens=148
16:39:00,707 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:00,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.895696546882391. input_tokens=347, output_tokens=101
16:39:01,234 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:01,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.275439988821745. input_tokens=631, output_tokens=291
16:39:01,381 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:01,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.49766418710351. input_tokens=348, output_tokens=79
16:39:01,609 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:01,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.022166961804032. input_tokens=387, output_tokens=172
16:39:01,991 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:01,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.051154039800167. input_tokens=333, output_tokens=106
16:39:02,35 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:02,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.745470667257905. input_tokens=370, output_tokens=100
16:39:02,52 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:02,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9174596089869738. input_tokens=357, output_tokens=107
16:39:02,85 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:02,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5354059413075447. input_tokens=335, output_tokens=87
16:39:02,217 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:02,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 8.359640836715698. input_tokens=617, output_tokens=279
16:39:02,596 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:02,597 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.794036211445928. input_tokens=359, output_tokens=85
16:39:02,691 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:02,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.829028973355889. input_tokens=407, output_tokens=149
16:39:03,79 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:03,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.385505495592952. input_tokens=355, output_tokens=94
16:39:03,214 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:03,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.883239613845944. input_tokens=342, output_tokens=145
16:39:03,224 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:03,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.11178688891232. input_tokens=400, output_tokens=187
16:39:03,982 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:03,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.745667601004243. input_tokens=397, output_tokens=121
16:39:04,0 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.009490432217717. input_tokens=409, output_tokens=179
16:39:04,146 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,149 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.223560392856598. input_tokens=356, output_tokens=166
16:39:04,206 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.387206584215164. input_tokens=308, output_tokens=110
16:39:04,527 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8179174829274416. input_tokens=401, output_tokens=127
16:39:04,671 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.104138307273388. input_tokens=360, output_tokens=152
16:39:04,985 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.479958359152079. input_tokens=363, output_tokens=111
16:39:05,42 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:05,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.874712944030762. input_tokens=354, output_tokens=172
16:39:05,441 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:05,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2262811083346605. input_tokens=379, output_tokens=84
16:39:05,627 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:05,631 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5435651764273643. input_tokens=360, output_tokens=127
16:39:05,750 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:05,755 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7587369978427887. input_tokens=346, output_tokens=105
16:39:05,796 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:05,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.202653469517827. input_tokens=361, output_tokens=98
16:39:06,204 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:06,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.292562356218696. input_tokens=962, output_tokens=418
16:39:06,263 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:06,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.0412222873419523. input_tokens=409, output_tokens=120
16:39:06,369 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:06,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.397885162383318. input_tokens=356, output_tokens=209
16:39:06,968 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:06,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7593077532947063. input_tokens=336, output_tokens=96
16:39:07,125 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:07,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.982054363936186. input_tokens=361, output_tokens=101
16:39:07,665 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:07,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.591298405081034. input_tokens=440, output_tokens=157
16:39:07,745 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:07,749 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.852439494803548. input_tokens=767, output_tokens=416
16:39:07,793 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:07,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.7521930038928986. input_tokens=353, output_tokens=86
16:39:08,328 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:08,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.6384840700775385. input_tokens=365, output_tokens=146
16:39:08,485 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:08,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.489739183336496. input_tokens=370, output_tokens=86
16:39:08,543 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:08,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.014165148139. input_tokens=411, output_tokens=138
16:39:08,578 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:08,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.348546100780368. input_tokens=489, output_tokens=289
16:39:08,923 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:08,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.9591242112219334. input_tokens=331, output_tokens=58
16:39:08,931 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:08,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.950222073122859. input_tokens=516, output_tokens=209
16:39:09,36 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.986000526696444. input_tokens=464, output_tokens=276
16:39:09,272 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.287435859441757. input_tokens=416, output_tokens=147
16:39:09,349 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9114000760018826. input_tokens=363, output_tokens=140
16:39:09,657 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4546797052025795. input_tokens=353, output_tokens=75
16:39:09,684 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.93235632404685. input_tokens=350, output_tokens=136
16:39:09,694 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.565287448465824. input_tokens=349, output_tokens=72
16:39:09,703 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 15.720103183761239. input_tokens=1229, output_tokens=580
16:39:09,776 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.103765435516834. input_tokens=349, output_tokens=139
16:39:09,835 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.8068699929863214. input_tokens=369, output_tokens=160
16:39:10,552 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:10,557 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.28746884688735. input_tokens=357, output_tokens=95
16:39:10,705 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:10,709 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9583337735384703. input_tokens=343, output_tokens=118
16:39:10,932 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:10,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.562803076580167. input_tokens=346, output_tokens=65
16:39:11,74 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:11,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4015695583075285. input_tokens=355, output_tokens=119
16:39:11,403 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:11,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.795754393562675. input_tokens=437, output_tokens=307
16:39:11,982 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:11,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.188917052000761. input_tokens=350, output_tokens=194
16:39:13,854 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:13,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.229775488376617. input_tokens=347, output_tokens=159
16:39:15,396 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:15,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.017960611730814. input_tokens=469, output_tokens=288
16:39:19,933 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:19,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 28.011845285072923. input_tokens=1027, output_tokens=595
16:39:20,489 graphrag.index.run.workflow INFO dependencies for create_final_covariates: ['create_base_text_units']
16:39:20,490 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:39:20,502 datashaper.workflow.workflow INFO executing verb create_final_covariates
16:39:20,819 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:39:20,820 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:39:20,821 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:39:20,824 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:39:20,826 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:39:20,827 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:39:26,478 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:26,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.8789252657443285. input_tokens=1871, output_tokens=114
16:39:28,358 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:28,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.823366709053516. input_tokens=2314, output_tokens=202
16:39:29,262 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:29,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 6.576416004449129. input_tokens=1150, output_tokens=121
16:39:29,409 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:29,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.86762760952115. input_tokens=2313, output_tokens=91
16:39:29,900 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:29,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.365283392369747. input_tokens=2314, output_tokens=199
16:39:30,137 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:30,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.580603908747435. input_tokens=2314, output_tokens=153
16:39:30,380 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:30,383 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.816643355414271. input_tokens=2314, output_tokens=217
16:39:31,897 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:31,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.32922181673348. input_tokens=1221, output_tokens=167
16:39:32,818 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:32,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 10.19747855886817. input_tokens=2314, output_tokens=130
16:39:34,407 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:34,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.793627213686705. input_tokens=1363, output_tokens=342
16:39:34,787 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:34,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.153022138401866. input_tokens=2314, output_tokens=114
16:39:36,228 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:36,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.703056512400508. input_tokens=2315, output_tokens=473
16:39:37,780 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:37,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 17.20388956181705. input_tokens=2314, output_tokens=326
16:39:40,39 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:40,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 19.4652543310076. input_tokens=2314, output_tokens=581
16:39:42,505 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:42,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.879452355206013. input_tokens=2314, output_tokens=687
16:39:42,972 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:42,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.38722687214613. input_tokens=2315, output_tokens=648
16:39:43,717 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:43,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.19570829719305. input_tokens=2314, output_tokens=640
16:39:45,537 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:45,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.98647902533412. input_tokens=1443, output_tokens=554
16:39:47,145 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:47,149 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.527477752417326. input_tokens=2250, output_tokens=635
16:39:49,737 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:49,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 27.20692046545446. input_tokens=2315, output_tokens=818
16:39:51,798 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:39:51,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.207193041220307. input_tokens=2313, output_tokens=746
16:40:10,232 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:10,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.99938494339585. input_tokens=19, output_tokens=906
16:40:11,595 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:11,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.99146378785372. input_tokens=2314, output_tokens=1063
16:40:12,948 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:12,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.049361577257514. input_tokens=19, output_tokens=870
16:40:14,203 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:14,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.939853604882956. input_tokens=19, output_tokens=795
16:40:14,211 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:14,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.8013004437089. input_tokens=19, output_tokens=905
16:40:15,615 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:15,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.57499683648348. input_tokens=19, output_tokens=1089
16:40:19,650 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:19,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.99946561269462. input_tokens=1429, output_tokens=1475
16:40:25,971 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:25,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.823239939287305. input_tokens=19, output_tokens=1204
16:40:29,434 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:29,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.05069884471595. input_tokens=19, output_tokens=1900
16:40:33,232 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:33,244 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.700445955619216. input_tokens=19, output_tokens=1262
16:40:38,933 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:38,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.42355550825596. input_tokens=19, output_tokens=1156
16:40:42,257 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:42,268 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.54401030018926. input_tokens=19, output_tokens=1474
16:40:46,33 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:46,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.30205815285444. input_tokens=19, output_tokens=1549
16:40:46,619 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:46,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 77.2057001106441. input_tokens=19, output_tokens=1313
16:40:46,787 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:46,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.88699321262538. input_tokens=19, output_tokens=1569
16:40:47,199 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:47,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 77.05878732353449. input_tokens=19, output_tokens=1686
16:40:49,4 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:49,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.0348901282996. input_tokens=19, output_tokens=1364
16:40:59,335 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:40:59,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.73732157610357. input_tokens=19, output_tokens=890
16:41:06,68 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:41:06,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 99.5855211596936. input_tokens=19, output_tokens=2680
16:41:18,871 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:41:18,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 101.08564681559801. input_tokens=19, output_tokens=1944
16:41:25,46 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:41:25,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 116.67948486283422. input_tokens=19, output_tokens=2645
16:41:34,891 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:41:34,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 103.0902802143246. input_tokens=19, output_tokens=1943
16:41:35,767 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:41:35,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 120.97596348635852. input_tokens=19, output_tokens=2505
16:41:40,486 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:41:40,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 127.65798083506525. input_tokens=19, output_tokens=3034
16:42:57,682 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:42:57,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 158.00635225139558. input_tokens=19, output_tokens=3219
16:42:57,708 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
16:42:58,103 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
16:42:58,103 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:42:58,114 datashaper.workflow.workflow INFO executing verb create_final_entities
16:42:58,228 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
16:42:58,556 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
16:42:58,557 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:42:58,569 datashaper.workflow.workflow INFO executing verb create_final_nodes
16:42:58,969 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
16:42:59,336 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
16:42:59,336 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:42:59,350 datashaper.workflow.workflow INFO executing verb create_final_communities
16:42:59,590 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
16:43:00,3 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
16:43:00,21 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
16:43:00,85 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:43:00,107 datashaper.workflow.workflow INFO executing verb create_final_relationships
16:43:00,233 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
16:43:00,583 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_covariates', 'create_base_text_units', 'create_final_entities']
16:43:00,584 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:43:00,597 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
16:43:00,610 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:43:00,611 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:43:00,643 datashaper.workflow.workflow INFO executing verb create_final_text_units
16:43:00,692 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
16:43:01,51 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_nodes', 'create_final_covariates', 'create_final_communities']
16:43:01,69 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:43:01,84 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
16:43:01,93 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
16:43:01,102 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
16:43:01,141 datashaper.workflow.workflow INFO executing verb create_final_community_reports
16:43:01,180 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=3 => 17
16:43:01,244 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=2 => 36
16:43:01,318 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 120
16:43:01,503 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 184
16:43:43,133 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:43:43,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.48372975178063. input_tokens=1777, output_tokens=1087
16:43:47,507 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:43:47,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.8621541839093. input_tokens=4316, output_tokens=1150
16:44:22,950 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:44:22,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.41252281703055. input_tokens=2191, output_tokens=1189
16:44:23,433 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:44:23,438 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.882716031745076. input_tokens=1917, output_tokens=884
16:44:29,333 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:44:29,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.78941140137613. input_tokens=5169, output_tokens=1414
16:44:36,650 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:44:36,658 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 49.09413588233292. input_tokens=4827, output_tokens=1322
16:45:05,35 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:05,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 28.340832078829408. input_tokens=1741, output_tokens=1004
16:45:08,362 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:08,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 31.63889066874981. input_tokens=2127, output_tokens=963
16:45:12,977 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:12,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.293620655313134. input_tokens=2716, output_tokens=1053
16:45:13,460 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:13,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.68900954909623. input_tokens=3004, output_tokens=1271
16:45:14,344 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:14,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.5668515432626. input_tokens=2629, output_tokens=1157
16:45:15,116 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:15,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.35645401850343. input_tokens=2194, output_tokens=1109
16:45:16,230 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:16,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.52493414841592. input_tokens=4954, output_tokens=1175
16:45:16,829 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:16,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.07457483187318. input_tokens=1742, output_tokens=1119
16:45:16,979 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:16,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.26429219357669. input_tokens=2163, output_tokens=1258
16:45:17,775 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:17,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.04638119228184. input_tokens=1956, output_tokens=1100
16:45:18,37 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:18,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.33788425475359. input_tokens=3835, output_tokens=1177
16:45:18,430 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:18,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.736806608736515. input_tokens=2484, output_tokens=1327
16:45:19,781 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:19,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.10079606436193. input_tokens=7277, output_tokens=1142
16:45:19,870 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:19,881 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.12015943787992. input_tokens=3942, output_tokens=1239
16:45:20,594 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:20,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.83101111650467. input_tokens=3815, output_tokens=1298
16:45:21,59 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:21,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 44.32482663728297. input_tokens=2395, output_tokens=1081
16:45:25,395 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:25,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.61044440791011. input_tokens=1981, output_tokens=1177
16:45:42,818 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:45:42,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 66.077388336882. input_tokens=7572, output_tokens=1503
16:46:16,184 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:16,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.22662881202996. input_tokens=2217, output_tokens=1021
16:46:20,313 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:20,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.37073634751141. input_tokens=8784, output_tokens=1070
16:46:22,687 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:22,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.70289129205048. input_tokens=1731, output_tokens=1252
16:46:24,74 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:24,78 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
16:46:24,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.08594078943133. input_tokens=3264, output_tokens=1375
16:46:24,150 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:24,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.17694243788719. input_tokens=3228, output_tokens=1067
16:46:32,365 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:32,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 49.3931411318481. input_tokens=7356, output_tokens=1550
16:46:32,733 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:32,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 49.778575422242284. input_tokens=4811, output_tokens=1230
16:46:35,100 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:35,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 52.13794613070786. input_tokens=2147, output_tokens=1422
16:46:40,991 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:41,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 58.00288364291191. input_tokens=3040, output_tokens=1298
16:46:51,449 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
16:46:51,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 68.44324833527207. input_tokens=8656, output_tokens=1954
16:46:51,477 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
16:46:51,919 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
16:46:51,929 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
16:46:51,974 datashaper.workflow.workflow INFO executing verb create_final_documents
16:46:51,993 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
16:46:52,352 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_community_reports', 'create_final_entities', 'create_final_relationships', 'create_final_text_units']
16:46:52,353 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
16:46:52,361 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
16:46:52,377 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:46:52,387 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:46:52,396 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
16:46:52,442 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
16:46:52,449 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
16:46:52,449 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
16:46:52,452 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
16:46:52,504 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-v1: TPM=0, RPM=0
16:46:52,504 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-v1: 25
16:46:52,547 graphrag.index.operations.embed_text.strategies.openai INFO embedding 34 inputs via 34 snippets using 5 batches. max_batch_size=16, max_tokens=8191
16:46:52,879 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:52,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.39410629495978355. input_tokens=4803, output_tokens=0
16:46:52,974 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:52,975 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:52,975 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:52,978 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:53,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.49060602113604546. input_tokens=7568, output_tokens=0
16:46:53,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5829581543803215. input_tokens=7529, output_tokens=0
16:46:53,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6834161262959242. input_tokens=7782, output_tokens=0
16:46:53,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.806250149384141. input_tokens=7786, output_tokens=0
16:46:53,466 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
16:46:53,494 graphrag.index.operations.embed_text.strategies.openai INFO embedding 23 inputs via 23 snippets using 3 batches. max_batch_size=16, max_tokens=8191
16:46:53,805 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:53,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3915368504822254. input_tokens=6539, output_tokens=0
16:46:53,913 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:53,914 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:54,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5170587003231049. input_tokens=7529, output_tokens=0
16:46:54,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6300634276121855. input_tokens=8062, output_tokens=0
16:46:54,209 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
16:46:54,231 graphrag.index.operations.embed_text.strategies.openai INFO embedding 184 inputs via 184 snippets using 12 batches. max_batch_size=16, max_tokens=8191
16:46:54,550 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:54,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.39531414210796356. input_tokens=363, output_tokens=0
16:46:54,671 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:54,672 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:54,673 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:54,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6172678377479315. input_tokens=1499, output_tokens=0
16:46:55,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8029796089977026. input_tokens=1021, output_tokens=0
16:46:55,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0676775332540274. input_tokens=1919, output_tokens=0
16:46:55,329 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,334 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,335 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,335 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,336 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,336 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,337 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,338 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
16:46:55,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2874206714332104. input_tokens=1160, output_tokens=0
16:46:55,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4749872330576181. input_tokens=1563, output_tokens=0
16:46:55,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6681152395904064. input_tokens=1773, output_tokens=0
16:46:56,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8494865521788597. input_tokens=1175, output_tokens=0
16:46:56,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0956507064402103. input_tokens=1100, output_tokens=0
16:46:56,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.309950703755021. input_tokens=994, output_tokens=0
16:46:56,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.5066101010888815. input_tokens=1208, output_tokens=0
16:46:57,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.755045600235462. input_tokens=1202, output_tokens=0
16:46:57,227 graphrag.cli.index INFO All workflows completed successfully.
17:09:29,180 graphrag.cli.index INFO Logging enabled at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs/indexing-engine.log
17:09:29,191 graphrag.cli.index INFO Starting pipeline run for: 20241125-170929, dry_run=False
17:09:29,192 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "qwen-max",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:13000/v1",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag",
    "reporting": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/update_output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-v1",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:09:29,196 graphrag.index.create_pipeline_config INFO skipping workflows 
17:09:29,196 graphrag.index.run.run INFO Running pipeline
17:09:29,196 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output
17:09:29,196 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/update_output
17:09:29,198 graphrag.index.input.load_input INFO loading input from root_dir=input
17:09:29,198 graphrag.index.input.load_input INFO using file storage for input
17:09:29,201 graphrag.index.storage.file_pipeline_storage INFO search /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/input for files matching .*\.txt$
17:09:29,202 graphrag.index.input.text INFO found text files from input, found [('6.txt', {}), ('3.txt', {}), ('7.txt', {}), ('1.txt', {}), ('9.txt', {}), ('2.txt', {}), ('5.txt', {}), ('8.txt', {}), ('4.txt', {})]
17:09:29,223 graphrag.index.input.text INFO Found 9 files, loading 9
17:09:29,224 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:09:29,256 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_covariates', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
17:09:29,256 graphrag.index.run.run INFO Final # of rows loaded: 3
17:09:29,617 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:09:29,624 datashaper.workflow.workflow INFO executing verb create_base_text_units
17:09:30,627 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
17:09:30,628 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:09:30,636 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
17:09:30,640 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:09:30,688 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for qwen-max: TPM=0, RPM=0
17:09:30,688 graphrag.index.llm.load_llm INFO create concurrency limiter for qwen-max: 25
17:09:39,136 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:09:39,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.402221204712987. input_tokens=1974, output_tokens=253
17:09:44,597 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:09:44,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 5.4463070426136255. input_tokens=34, output_tokens=155
17:09:58,529 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:09:58,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.790499033406377. input_tokens=3088, output_tokens=824
17:10:02,489 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:02,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.75786881148815. input_tokens=2267, output_tokens=913
17:10:03,641 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:03,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.940923465415835. input_tokens=3088, output_tokens=1110
17:10:07,174 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:07,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.46066763624549. input_tokens=2327, output_tokens=1106
17:10:11,229 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:11,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.5142938029021. input_tokens=3088, output_tokens=1263
17:10:14,187 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:14,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.542071668431163. input_tokens=34, output_tokens=329
17:10:17,873 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:17,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.341546973213553. input_tokens=34, output_tokens=463
17:10:18,162 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:18,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.436160143464804. input_tokens=3088, output_tokens=1329
17:10:20,840 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:20,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.115215480327606. input_tokens=3087, output_tokens=1270
17:10:23,133 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:23,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.63573870807886. input_tokens=34, output_tokens=416
17:10:23,365 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:23,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.62463418394327. input_tokens=3074, output_tokens=1515
17:10:24,552 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:24,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.375216642394662. input_tokens=34, output_tokens=559
17:10:31,22 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:31,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.788132589310408. input_tokens=34, output_tokens=601
17:10:39,330 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:39,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.489492513239384. input_tokens=34, output_tokens=605
17:10:41,708 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:41,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.337436636909842. input_tokens=34, output_tokens=554
17:10:51,551 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:51,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.384351283311844. input_tokens=34, output_tokens=580
17:10:51,843 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:10:51,845 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:10:51,847 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:10:51,848 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:10:51,849 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:10:51,850 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:10:51,851 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:10:51,852 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:10:51,857 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:10:51,860 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:10:54,214 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:54,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5388267673552036. input_tokens=342, output_tokens=93
17:10:54,380 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:54,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.689222764223814. input_tokens=307, output_tokens=58
17:10:54,490 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:54,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8023343551903963. input_tokens=342, output_tokens=109
17:10:55,254 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:55,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5909351520240307. input_tokens=372, output_tokens=122
17:10:55,308 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:55,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6396671049296856. input_tokens=359, output_tokens=116
17:10:56,308 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:56,316 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 3.2187306974083185. input_tokens=344, output_tokens=128
17:10:56,384 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:56,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.730992861092091. input_tokens=379, output_tokens=138
17:10:56,516 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:56,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.85910077393055. input_tokens=387, output_tokens=183
17:10:56,572 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:56,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.865395303815603. input_tokens=360, output_tokens=155
17:10:56,621 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:56,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.941137796267867. input_tokens=322, output_tokens=105
17:10:56,832 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:56,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.189600706100464. input_tokens=382, output_tokens=146
17:10:57,41 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 4.067757710814476. input_tokens=336, output_tokens=155
17:10:57,97 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.462813265621662. input_tokens=375, output_tokens=138
17:10:57,127 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 3.5067189801484346. input_tokens=364, output_tokens=125
17:10:57,235 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.850820643827319. input_tokens=361, output_tokens=99
17:10:57,486 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.8036890756338835. input_tokens=308, output_tokens=177
17:10:57,624 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.953925231471658. input_tokens=443, output_tokens=210
17:10:57,895 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 4.957470128312707. input_tokens=337, output_tokens=92
17:10:57,948 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:57,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4549336433410645. input_tokens=353, output_tokens=118
17:10:58,492 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:58,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.855690125375986. input_tokens=346, output_tokens=193
17:10:58,685 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:58,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.995469240471721. input_tokens=402, output_tokens=207
17:10:58,749 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:58,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.442658880725503. input_tokens=328, output_tokens=123
17:10:59,890 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:10:59,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.636914309114218. input_tokens=325, output_tokens=113
17:11:00,237 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:00,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.923790916800499. input_tokens=342, output_tokens=121
17:11:00,285 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:00,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 6.521268265321851. input_tokens=404, output_tokens=216
17:11:00,454 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:00,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.74619685485959. input_tokens=481, output_tokens=270
17:11:00,692 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:00,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.076035792008042. input_tokens=575, output_tokens=298
17:11:01,171 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:01,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.471214769408107. input_tokens=411, output_tokens=206
17:11:01,790 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:01,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.16521685756743. input_tokens=665, output_tokens=305
17:11:01,815 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:01,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.59803426079452. input_tokens=437, output_tokens=232
17:11:02,752 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:02,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.130081169307232. input_tokens=915, output_tokens=416
17:11:03,122 graphrag.index.run.workflow INFO dependencies for create_final_covariates: ['create_base_text_units']
17:11:03,123 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:11:03,132 datashaper.workflow.workflow INFO executing verb create_final_covariates
17:11:09,52 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:09,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.872071390971541. input_tokens=1495, output_tokens=149
17:11:09,81 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:09,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 5.933489371091127. input_tokens=2314, output_tokens=103
17:11:09,612 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:09,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.443626809865236. input_tokens=2314, output_tokens=130
17:11:10,174 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:10,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.987232865765691. input_tokens=2302, output_tokens=103
17:11:11,312 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:11,320 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 8.155730627477169. input_tokens=2313, output_tokens=140
17:11:12,630 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:12,633 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.464120844379067. input_tokens=1555, output_tokens=120
17:11:13,349 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:13,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.162175199016929. input_tokens=1202, output_tokens=127
17:11:23,955 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:23,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.779048705473542. input_tokens=2314, output_tokens=482
17:11:28,896 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:28,901 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.741419170051813. input_tokens=2314, output_tokens=565
17:11:40,34 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:40,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.975189989432693. input_tokens=19, output_tokens=779
17:11:54,349 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:11:54,357 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.992388581857085. input_tokens=19, output_tokens=838
17:12:21,363 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:12:21,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.401954589411616. input_tokens=19, output_tokens=1331
17:12:23,975 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:12:23,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.33954550884664. input_tokens=19, output_tokens=1682
17:12:42,610 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:12:42,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 92.9927881732583. input_tokens=19, output_tokens=2646
17:12:45,99 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:12:45,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 94.92190445773304. input_tokens=19, output_tokens=2711
17:12:54,62 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:12:54,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 102.73875612206757. input_tokens=19, output_tokens=2811
17:12:55,194 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:12:55,199 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 106.11055541969836. input_tokens=19, output_tokens=2672
17:13:08,424 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:13:08,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 99.52300809882581. input_tokens=19, output_tokens=2438
17:13:08,450 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
17:13:08,748 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:13:08,749 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:13:08,760 datashaper.workflow.workflow INFO executing verb create_final_entities
17:13:08,784 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:13:09,76 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:13:09,83 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:13:09,111 datashaper.workflow.workflow INFO executing verb create_final_nodes
17:13:09,198 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:13:09,487 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:13:09,488 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:13:09,503 datashaper.workflow.workflow INFO executing verb create_final_communities
17:13:09,589 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:13:09,895 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
17:13:09,896 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:13:09,933 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:13:09,955 datashaper.workflow.workflow INFO executing verb create_final_relationships
17:13:09,989 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:13:10,280 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_covariates', 'create_base_text_units', 'create_final_relationships']
17:13:10,297 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:13:10,315 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:13:10,331 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:13:10,332 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:13:10,368 datashaper.workflow.workflow INFO executing verb create_final_text_units
17:13:10,429 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:13:10,714 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_covariates', 'create_final_nodes', 'create_final_communities', 'create_final_relationships']
17:13:10,714 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:13:10,724 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:13:10,733 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:13:10,741 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:13:10,770 datashaper.workflow.workflow INFO executing verb create_final_community_reports
17:13:10,796 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 16
17:13:10,861 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 61
17:13:44,820 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:13:44,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 33.85442276671529. input_tokens=2128, output_tokens=1204
17:13:56,408 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:13:56,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.44718034379184. input_tokens=3677, output_tokens=1327
17:14:07,202 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:07,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 56.24967513233423. input_tokens=2526, output_tokens=1562
17:14:32,689 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:32,695 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:14:32,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 25.4198974724859. input_tokens=1542, output_tokens=787
17:14:37,141 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:37,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.855567045509815. input_tokens=1720, output_tokens=1078
17:14:39,656 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:39,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 32.3898920211941. input_tokens=6442, output_tokens=1118
17:14:43,151 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:43,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.85184303857386. input_tokens=3407, output_tokens=1218
17:14:45,245 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:45,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.95048877969384. input_tokens=3298, output_tokens=1095
17:14:49,302 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:49,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.997981395572424. input_tokens=3124, output_tokens=1099
17:14:53,524 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:14:53,532 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.214960956946015. input_tokens=2615, output_tokens=1286
17:15:01,689 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:01,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 54.441659931093454. input_tokens=2658, output_tokens=1283
17:15:01,717 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:15:02,18 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
17:15:02,19 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:15:02,55 datashaper.workflow.workflow INFO executing verb create_final_documents
17:15:02,73 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:15:02,371 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_community_reports', 'create_final_documents', 'create_final_relationships', 'create_final_text_units', 'create_final_entities']
17:15:02,382 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:15:02,405 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:15:02,412 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:15:02,420 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:15:02,427 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:15:02,474 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
17:15:02,481 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:15:02,481 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:15:02,483 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:15:02,537 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-v1: TPM=0, RPM=0
17:15:02,537 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-v1: 25
17:15:02,545 graphrag.index.operations.embed_text.strategies.openai INFO embedding 9 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
17:15:02,910 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:03,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.47307249158620834. input_tokens=8100, output_tokens=0
17:15:03,105 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:15:03,119 graphrag.index.operations.embed_text.strategies.openai INFO embedding 11 inputs via 11 snippets using 2 batches. max_batch_size=16, max_tokens=8191
17:15:03,339 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:03,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.2512905206531286. input_tokens=3051, output_tokens=0
17:15:03,437 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:03,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.42508753947913647. input_tokens=8061, output_tokens=0
17:15:03,627 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:15:03,640 graphrag.index.operations.embed_text.strategies.openai INFO embedding 61 inputs via 61 snippets using 4 batches. max_batch_size=16, max_tokens=8191
17:15:03,908 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:04,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43038378469645977. input_tokens=851, output_tokens=0
17:15:04,114 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:04,114 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:04,115 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:04,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6630452089011669. input_tokens=2398, output_tokens=0
17:15:04,517 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.865019217133522. input_tokens=992, output_tokens=0
17:15:04,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0546231493353844. input_tokens=1355, output_tokens=0
17:15:04,834 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:15:04,871 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:15:08,100 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:08,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1328840609639883. input_tokens=336, output_tokens=103
17:15:09,875 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:09,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.8924212120473385. input_tokens=319, output_tokens=114
17:15:10,488 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:10,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.5086110550910234. input_tokens=303, output_tokens=167
17:15:10,931 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:10,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.971773788332939. input_tokens=484, output_tokens=227
17:15:11,620 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:11,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.649430479854345. input_tokens=441, output_tokens=227
17:15:12,218 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:12,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.268746664747596. input_tokens=483, output_tokens=258
17:15:12,526 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:12,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.55285900272429. input_tokens=367, output_tokens=151
17:15:12,692 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:12,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.701812310144305. input_tokens=538, output_tokens=227
17:15:20,392 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:20,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.452511230483651. input_tokens=975, output_tokens=547
17:15:24,288 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:24,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 19.332193419337273. input_tokens=918, output_tokens=638
17:15:25,32 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:25,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 20.047304671257734. input_tokens=866, output_tokens=531
17:15:27,716 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:15:27,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 22.751275539398193. input_tokens=1256, output_tokens=816
17:15:27,748 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:15:27,785 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:15:27,829 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:15:27,885 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:15:27,981 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:15:28,24 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:15:28,100 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:15:28,100 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:15:28,141 graphrag.index.operations.embed_text.strategies.openai INFO embedding 32 inputs via 32 snippets using 4 batches. max_batch_size=16, max_tokens=8191
17:15:28,494 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:28,496 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:28,576 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.41961629316210747. input_tokens=6458, output_tokens=0
17:15:28,699 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5481050349771976. input_tokens=8181, output_tokens=0
17:15:28,756 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:15:28,810 graphrag.index.operations.embed_text.strategies.openai INFO embedding 45 inputs via 45 snippets using 7 batches. max_batch_size=16, max_tokens=8191
17:15:29,64 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.25928991474211216. input_tokens=1108, output_tokens=0
17:15:29,88 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,195 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.37703921273350716. input_tokens=7333, output_tokens=0
17:15:29,209 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5061923693865538. input_tokens=7474, output_tokens=0
17:15:29,378 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:15:29,403 graphrag.index.operations.embed_text.strategies.openai INFO embedding 233 inputs via 233 snippets using 15 batches. max_batch_size=16, max_tokens=8191
17:15:29,708 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3996692467480898. input_tokens=1087, output_tokens=0
17:15:29,861 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,863 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,880 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,882 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,882 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,883 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,885 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,885 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,886 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,892 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,894 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,909 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:29,920 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:30,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7512344755232334. input_tokens=1163, output_tokens=0
17:15:30,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9524206109344959. input_tokens=1370, output_tokens=0
17:15:30,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1336901020258665. input_tokens=1550, output_tokens=0
17:15:30,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3403694033622742. input_tokens=1283, output_tokens=0
17:15:30,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5567407123744488. input_tokens=1142, output_tokens=0
17:15:31,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7822691593319178. input_tokens=1785, output_tokens=0
17:15:31,227 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:15:31,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9888287615031004. input_tokens=1219, output_tokens=0
17:15:31,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.259741447865963. input_tokens=1848, output_tokens=0
17:15:31,878 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.460110129788518. input_tokens=1106, output_tokens=0
17:15:32,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.675821928307414. input_tokens=2025, output_tokens=0
17:15:32,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.9517852757126093. input_tokens=1067, output_tokens=0
17:15:32,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.19980226457119. input_tokens=1366, output_tokens=0
17:15:32,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.42957373149693. input_tokens=1293, output_tokens=0
17:15:33,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.6635452024638653. input_tokens=1066, output_tokens=0
17:15:33,298 graphrag.cli.index INFO All workflows completed successfully.
17:31:25,181 graphrag.cli.index INFO Logging enabled at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs/indexing-engine.log
17:31:25,204 graphrag.cli.index INFO Starting pipeline run for: 20241125-173125, dry_run=False
17:31:25,208 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "qwen-max",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:13000/v1",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag",
    "reporting": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/update_output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-v1",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:31:25,210 graphrag.index.create_pipeline_config INFO skipping workflows 
17:31:25,211 graphrag.index.run.run INFO Running pipeline
17:31:25,211 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output
17:31:25,211 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/update_output
17:31:25,212 graphrag.index.input.load_input INFO loading input from root_dir=input
17:31:25,212 graphrag.index.input.load_input INFO using file storage for input
17:31:25,213 graphrag.index.storage.file_pipeline_storage INFO search /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/input for files matching .*\.txt$
17:31:25,214 graphrag.index.input.text INFO found text files from input, found [('6.txt', {}), ('3.txt', {}), ('7.txt', {}), ('1.txt', {}), ('9.txt', {}), ('2.txt', {}), ('5.txt', {}), ('8.txt', {}), ('4.txt', {})]
17:31:25,224 graphrag.index.input.text INFO Found 9 files, loading 9
17:31:25,226 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:31:25,259 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_covariates', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
17:31:25,260 graphrag.index.run.run INFO Final # of rows loaded: 3
17:31:25,607 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:31:25,617 datashaper.workflow.workflow INFO executing verb create_base_text_units
17:31:26,499 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
17:31:26,499 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:31:26,508 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
17:31:26,512 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:31:26,558 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for qwen-max: TPM=0, RPM=0
17:31:26,558 graphrag.index.llm.load_llm INFO create concurrency limiter for qwen-max: 25
17:31:39,176 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:31:39,189 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.572621796280146. input_tokens=2027, output_tokens=293
17:31:46,534 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:31:46,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.346885673701763. input_tokens=34, output_tokens=146
17:31:53,711 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:31:53,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.106067180633545. input_tokens=3141, output_tokens=941
17:32:02,82 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:02,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.46273451298475. input_tokens=3127, output_tokens=1233
17:32:05,103 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:05,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.51707944832742. input_tokens=3141, output_tokens=1186
17:32:06,886 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:06,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.29430401697755. input_tokens=3140, output_tokens=1218
17:32:09,158 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:09,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.591666819527745. input_tokens=3141, output_tokens=1388
17:32:09,471 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:09,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.86672685481608. input_tokens=2320, output_tokens=1154
17:32:12,443 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:12,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.724839374423027. input_tokens=34, output_tokens=519
17:32:15,265 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:15,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.69220854341984. input_tokens=2380, output_tokens=1379
17:32:18,460 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:18,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.982511902227998. input_tokens=34, output_tokens=263
17:32:19,312 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:19,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.71153108216822. input_tokens=3141, output_tokens=1534
17:32:24,536 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:24,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.422411972656846. input_tokens=34, output_tokens=538
17:32:32,802 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:32,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.715841576457024. input_tokens=34, output_tokens=893
17:32:38,920 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:38,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.6039256490767. input_tokens=34, output_tokens=626
17:32:39,752 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:39,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.478709030896425. input_tokens=34, output_tokens=681
17:32:41,24 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:41,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.871922651305795. input_tokens=34, output_tokens=761
17:32:49,926 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:49,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.031411619856954. input_tokens=34, output_tokens=924
17:32:50,322 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:32:50,324 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:32:50,326 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:32:50,327 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:32:50,328 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:32:50,329 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:32:50,331 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:32:50,333 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:32:50,334 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:32:50,336 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:32:53,388 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.291952833533287. input_tokens=301, output_tokens=92
17:32:53,509 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.413368320092559. input_tokens=325, output_tokens=124
17:32:53,560 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4702418353408575. input_tokens=374, output_tokens=151
17:32:53,609 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 1.4581641405820847. input_tokens=316, output_tokens=62
17:32:53,633 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.557689242064953. input_tokens=356, output_tokens=103
17:32:53,808 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7232411354780197. input_tokens=307, output_tokens=99
17:32:54,1 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:54,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9205686822533607. input_tokens=352, output_tokens=125
17:32:54,29 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:54,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.9266977589577436. input_tokens=295, output_tokens=118
17:32:54,186 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:54,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.051606701686978. input_tokens=362, output_tokens=131
17:32:54,444 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:54,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.3156768418848515. input_tokens=338, output_tokens=158
17:32:55,38 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.92697991989553. input_tokens=423, output_tokens=189
17:32:55,50 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.9707011859864. input_tokens=369, output_tokens=219
17:32:55,322 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,329 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.1411501336842775. input_tokens=457, output_tokens=207
17:32:55,422 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.278727175667882. input_tokens=443, output_tokens=205
17:32:55,431 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 3.2398794181644917. input_tokens=354, output_tokens=112
17:32:55,619 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.228899732232094. input_tokens=331, output_tokens=84
17:32:56,67 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:56,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.944066600874066. input_tokens=422, output_tokens=174
17:32:56,675 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:56,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.349677324295044. input_tokens=299, output_tokens=37
17:32:56,730 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:56,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.570788778364658. input_tokens=375, output_tokens=238
17:32:56,815 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:56,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.747287241742015. input_tokens=446, output_tokens=260
17:32:56,885 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:56,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4393592327833176. input_tokens=321, output_tokens=91
17:32:56,978 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:56,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.469903351739049. input_tokens=381, output_tokens=179
17:32:57,3 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9780312571674585. input_tokens=315, output_tokens=91
17:32:57,91 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 5.152755504474044. input_tokens=333, output_tokens=145
17:32:57,141 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5800937674939632. input_tokens=380, output_tokens=170
17:32:57,356 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 5.115511205047369. input_tokens=386, output_tokens=162
17:32:57,610 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.559778992086649. input_tokens=622, output_tokens=294
17:32:57,969 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9362121894955635. input_tokens=347, output_tokens=102
17:32:58,186 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:58,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.377034809440374. input_tokens=353, output_tokens=160
17:32:58,402 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:58,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.203877745196223. input_tokens=309, output_tokens=194
17:32:58,682 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:58,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 6.563795622438192. input_tokens=343, output_tokens=163
17:32:58,812 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:58,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.205943718552589. input_tokens=401, output_tokens=236
17:32:58,979 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:58,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.930375227704644. input_tokens=340, output_tokens=77
17:32:59,549 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:32:59,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.919391700997949. input_tokens=322, output_tokens=104
17:33:01,985 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:01,990 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.85458262078464. input_tokens=347, output_tokens=100
17:33:02,6 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:02,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.002419577911496. input_tokens=337, output_tokens=169
17:33:04,729 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:04,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.668437872081995. input_tokens=1017, output_tokens=488
17:33:06,239 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:06,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.173521215096116. input_tokens=754, output_tokens=337
17:33:06,650 graphrag.index.run.workflow INFO dependencies for create_final_covariates: ['create_base_text_units']
17:33:06,651 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:33:06,660 datashaper.workflow.workflow INFO executing verb create_final_covariates
17:33:06,708 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
17:33:07,17 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:33:07,19 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:33:07,40 datashaper.workflow.workflow INFO executing verb create_final_entities
17:33:07,70 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:33:07,421 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:33:07,436 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:33:07,459 datashaper.workflow.workflow INFO executing verb create_final_nodes
17:33:07,574 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:33:07,890 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:33:07,891 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:33:07,905 datashaper.workflow.workflow INFO executing verb create_final_communities
17:33:07,994 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:33:08,285 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
17:33:08,286 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:33:08,287 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:33:08,369 datashaper.workflow.workflow INFO executing verb create_final_relationships
17:33:08,402 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:33:08,704 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_covariates', 'create_final_entities']
17:33:08,715 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:33:08,733 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:33:08,734 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:33:08,745 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:33:08,780 datashaper.workflow.workflow INFO executing verb create_final_text_units
17:33:08,842 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:33:09,139 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_communities', 'create_final_nodes', 'create_final_covariates', 'create_final_relationships']
17:33:09,140 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:33:09,148 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:33:09,162 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:33:09,177 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:33:09,211 datashaper.workflow.workflow INFO executing verb create_final_community_reports
17:33:09,230 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 31
17:33:09,337 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 75
17:33:38,700 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:38,704 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.238787906244397. input_tokens=2101, output_tokens=1010
17:33:44,204 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:44,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.762651627883315. input_tokens=2762, output_tokens=1193
17:33:44,857 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:44,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.401317210868. input_tokens=4022, output_tokens=1297
17:33:45,573 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:33:45,577 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:33:45,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.13971454836428. input_tokens=3121, output_tokens=1228
17:34:05,539 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:05,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 56.09083575569093. input_tokens=2668, output_tokens=1252
17:34:36,475 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:36,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.896762946620584. input_tokens=1820, output_tokens=1054
17:34:39,646 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:39,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.06446419470012. input_tokens=4707, output_tokens=1267
17:34:40,310 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:40,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.742593698203564. input_tokens=2573, output_tokens=1218
17:34:42,75 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:42,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.50573120824993. input_tokens=2901, output_tokens=1301
17:34:46,379 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:46,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.782716097310185. input_tokens=3282, output_tokens=1408
17:34:50,619 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:50,623 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:34:50,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.008713668212295. input_tokens=6311, output_tokens=1298
17:34:50,650 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:34:50,956 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
17:34:50,973 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:34:51,6 datashaper.workflow.workflow INFO executing verb create_final_documents
17:34:51,33 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:34:51,340 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_text_units', 'create_final_relationships', 'create_final_entities', 'create_final_documents', 'create_final_community_reports']
17:34:51,341 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:34:51,352 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:34:51,359 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:34:51,370 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:34:51,376 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:34:51,409 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
17:34:51,415 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:34:51,415 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:34:51,417 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:34:51,488 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-v1: TPM=0, RPM=0
17:34:51,489 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-v1: 25
17:34:51,497 graphrag.index.operations.embed_text.strategies.openai INFO embedding 9 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
17:34:51,578 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:34:51,594 graphrag.index.operations.embed_text.strategies.openai INFO embedding 11 inputs via 11 snippets using 2 batches. max_batch_size=16, max_tokens=8191
17:34:51,872 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:51,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.3226130735129118. input_tokens=4381, output_tokens=0
17:34:51,945 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:52,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.4402787405997515. input_tokens=7318, output_tokens=0
17:34:52,112 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:34:52,123 graphrag.index.operations.embed_text.strategies.openai INFO embedding 75 inputs via 75 snippets using 5 batches. max_batch_size=16, max_tokens=8191
17:34:52,451 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:52,456 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:52,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5071794521063566. input_tokens=1356, output_tokens=0
17:34:52,660 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:52,660 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:52,661 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:34:52,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6470347046852112. input_tokens=544, output_tokens=0
17:34:52,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8389051109552383. input_tokens=2615, output_tokens=0
17:34:53,209 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0767343137413263. input_tokens=1092, output_tokens=0
17:34:53,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.27608591504395. input_tokens=1229, output_tokens=0
17:34:53,522 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:34:53,558 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:34:57,152 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:57,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4545315392315388. input_tokens=375, output_tokens=126
17:34:57,404 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:57,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7047716435045004. input_tokens=324, output_tokens=157
17:34:57,598 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:34:57,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.90581626445055. input_tokens=384, output_tokens=141
17:35:01,882 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:35:01,888 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.20119572430849. input_tokens=558, output_tokens=294
17:35:02,23 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:35:02,29 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.318130621686578. input_tokens=599, output_tokens=323
17:35:05,721 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:35:05,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.020576231181622. input_tokens=882, output_tokens=483
17:35:07,201 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:35:07,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.521199516952038. input_tokens=568, output_tokens=281
17:35:07,402 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:35:07,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.724903229624033. input_tokens=951, output_tokens=597
17:35:14,223 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:35:14,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 20.54163049161434. input_tokens=1308, output_tokens=743
17:35:14,260 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:35:14,305 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:35:14,339 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:35:14,390 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:35:14,517 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:35:14,561 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:35:14,624 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:35:14,624 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:35:14,703 graphrag.index.operations.embed_text.strategies.openai INFO embedding 32 inputs via 32 snippets using 4 batches. max_batch_size=16, max_tokens=8191
17:35:14,801 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:35:14,856 graphrag.index.operations.embed_text.strategies.openai INFO embedding 45 inputs via 45 snippets using 7 batches. max_batch_size=16, max_tokens=8191
17:35:14,998 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 400 Bad Request"
17:38:46,580 graphrag.cli.index INFO Logging enabled at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs/indexing-engine.log
17:38:46,587 graphrag.cli.index INFO Starting pipeline run for: 20241125-173846, dry_run=False
17:38:46,589 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "qwen-max",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:13000/v1",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag",
    "reporting": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/update_output",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-v1",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:38:46,592 graphrag.index.create_pipeline_config INFO skipping workflows 
17:38:46,592 graphrag.index.run.run INFO Running pipeline
17:38:46,592 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output
17:38:46,593 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/update_output
17:38:46,593 graphrag.index.input.load_input INFO loading input from root_dir=input
17:38:46,593 graphrag.index.input.load_input INFO using file storage for input
17:38:46,595 graphrag.index.storage.file_pipeline_storage INFO search /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/input for files matching .*\.txt$
17:38:46,595 graphrag.index.input.text INFO found text files from input, found [('6.txt', {}), ('3.txt', {}), ('7.txt', {}), ('1.txt', {}), ('9.txt', {}), ('2.txt', {}), ('5.txt', {}), ('8.txt', {}), ('4.txt', {})]
17:38:46,607 graphrag.index.input.text INFO Found 9 files, loading 9
17:38:46,608 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:38:46,644 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_covariates', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
17:38:46,645 graphrag.index.run.run INFO Final # of rows loaded: 3
17:38:46,948 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:38:46,955 datashaper.workflow.workflow INFO executing verb create_base_text_units
17:38:47,909 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
17:38:47,910 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:47,920 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
17:38:47,924 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:38:47,969 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for qwen-max: TPM=0, RPM=0
17:38:47,970 graphrag.index.llm.load_llm INFO create concurrency limiter for qwen-max: 25
17:38:48,515 graphrag.index.run.workflow INFO dependencies for create_final_covariates: ['create_base_text_units']
17:38:48,516 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:48,526 datashaper.workflow.workflow INFO executing verb create_final_covariates
17:38:48,571 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
17:38:48,889 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:38:48,900 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:48,915 datashaper.workflow.workflow INFO executing verb create_final_entities
17:38:48,941 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:38:49,234 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:38:49,235 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:49,246 datashaper.workflow.workflow INFO executing verb create_final_nodes
17:38:49,361 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:38:49,655 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:38:49,656 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:49,670 datashaper.workflow.workflow INFO executing verb create_final_communities
17:38:49,790 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:38:50,142 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
17:38:50,154 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:50,161 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:38:50,248 datashaper.workflow.workflow INFO executing verb create_final_relationships
17:38:50,287 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:38:50,617 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_base_text_units', 'create_final_entities', 'create_final_relationships', 'create_final_covariates']
17:38:50,618 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:38:50,619 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:38:50,627 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:38:50,635 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:38:50,674 datashaper.workflow.workflow INFO executing verb create_final_text_units
17:38:50,718 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:38:51,30 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_communities', 'create_final_relationships', 'create_final_nodes', 'create_final_covariates']
17:38:51,30 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:38:51,38 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:38:51,47 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:38:51,60 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:38:51,91 datashaper.workflow.workflow INFO executing verb create_final_community_reports
17:38:51,130 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 31
17:38:51,249 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 75
17:38:51,416 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:38:51,720 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
17:38:51,721 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:38:51,753 datashaper.workflow.workflow INFO executing verb create_final_documents
17:38:51,780 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:38:52,77 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_entities', 'create_final_text_units', 'create_final_relationships', 'create_final_community_reports']
17:38:52,78 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:38:52,85 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:38:52,97 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:38:52,106 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:38:52,114 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:38:52,164 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
17:38:52,171 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:38:52,171 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:38:52,173 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:38:52,227 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-v1: TPM=0, RPM=0
17:38:52,227 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-v1: 25
17:38:52,258 graphrag.index.operations.embed_text.strategies.openai INFO embedding 11 inputs via 11 snippets using 2 batches. max_batch_size=16, max_tokens=8191
17:38:52,367 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:38:52,377 graphrag.index.operations.embed_text.strategies.openai INFO embedding 9 inputs via 9 snippets using 1 batches. max_batch_size=16, max_tokens=8191
17:38:52,434 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:38:52,445 graphrag.index.operations.embed_text.strategies.openai INFO embedding 75 inputs via 75 snippets using 5 batches. max_batch_size=16, max_tokens=8191
17:38:52,720 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:38:52,748 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:38:52,903 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:38:52,947 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:38:53,1 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:38:53,44 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:38:53,135 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:38:53,167 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:38:53,239 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:38:53,239 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:38:53,324 graphrag.index.operations.embed_text.strategies.openai INFO embedding 45 inputs via 45 snippets using 7 batches. max_batch_size=16, max_tokens=8191
17:38:53,696 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:53,721 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.38216138258576393. input_tokens=1151, output_tokens=0
17:38:53,863 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:53,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5926094949245453. input_tokens=7601, output_tokens=0
17:38:53,952 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,25 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6896943114697933. input_tokens=7750, output_tokens=0
17:38:54,76 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:38:54,106 graphrag.index.operations.embed_text.strategies.openai INFO embedding 32 inputs via 32 snippets using 4 batches. max_batch_size=16, max_tokens=8191
17:38:54,194 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:38:54,223 graphrag.index.operations.embed_text.strategies.openai INFO embedding 250 inputs via 250 snippets using 16 batches. max_batch_size=16, max_tokens=8191
17:38:54,593 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5540801826864481. input_tokens=1128, output_tokens=0
17:38:54,812 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,813 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,813 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,814 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,814 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,815 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,816 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,816 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,817 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,817 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,818 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,818 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,819 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,820 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,820 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:38:54,955 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6749716941267252. input_tokens=1073, output_tokens=0
17:38:55,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8996894974261522. input_tokens=1371, output_tokens=0
17:38:55,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1193407028913498. input_tokens=975, output_tokens=0
17:38:55,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3337034545838833. input_tokens=1336, output_tokens=0
17:38:55,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.523824380710721. input_tokens=1795, output_tokens=0
17:38:55,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6808833088725805. input_tokens=1195, output_tokens=0
17:38:56,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9125359244644642. input_tokens=1123, output_tokens=0
17:38:56,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1279739551246166. input_tokens=1244, output_tokens=0
17:38:56,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.339988552033901. input_tokens=1770, output_tokens=0
17:38:56,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.5294247325509787. input_tokens=889, output_tokens=0
17:38:56,984 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.7488824147731066. input_tokens=1298, output_tokens=0
17:38:57,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.95162177272141. input_tokens=1229, output_tokens=0
17:38:57,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.2371068634092808. input_tokens=1302, output_tokens=0
17:38:57,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.468076329678297. input_tokens=1755, output_tokens=0
17:38:57,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.6764078810811043. input_tokens=1658, output_tokens=0
17:38:58,122 graphrag.cli.index INFO All workflows completed successfully.
17:46:31,842 graphrag.cli.index INFO Logging enabled at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs/indexing-engine.log
17:46:31,847 graphrag.cli.index INFO Starting pipeline run for: 20241125-174631, dry_run=False
17:46:31,848 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "qwen-max",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://localhost:13000/v1",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag",
    "reporting": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-v1",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "qwen-max",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://localhost:13000/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": true,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:46:31,850 graphrag.index.create_pipeline_config INFO skipping workflows 
17:46:31,851 graphrag.index.run.run INFO Running pipeline
17:46:31,851 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/output
17:46:31,851 graphrag.index.input.load_input INFO loading input from root_dir=input
17:46:31,851 graphrag.index.input.load_input INFO using file storage for input
17:46:31,854 graphrag.index.storage.file_pipeline_storage INFO search /home/ggg/project/new/Innovation-Project/main/graphrag/graphrag/input for files matching .*\.txt$
17:46:31,854 graphrag.index.input.text INFO found text files from input, found [('6.txt', {}), ('3.txt', {}), ('7.txt', {}), ('1.txt', {}), ('9.txt', {}), ('2.txt', {}), ('5.txt', {}), ('8.txt', {}), ('4.txt', {})]
17:46:31,867 graphrag.index.input.text INFO Found 9 files, loading 9
17:46:31,870 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_entity_graph', 'create_final_covariates', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'create_final_documents', 'generate_text_embeddings']
17:46:31,871 graphrag.index.run.run INFO Final # of rows loaded: 9
17:46:32,182 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:46:32,188 datashaper.workflow.workflow INFO executing verb create_base_text_units
17:46:33,83 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
17:46:33,87 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:46:33,99 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
17:46:33,110 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:46:33,176 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for qwen-max: TPM=0, RPM=0
17:46:33,176 graphrag.index.llm.load_llm INFO create concurrency limiter for qwen-max: 25
17:46:33,539 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:46:33,542 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:46:33,543 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:46:33,544 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:46:33,544 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:46:33,547 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:46:33,550 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:46:33,551 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:46:33,553 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:46:33,555 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:46:42,630 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:46:42,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.359606800600886. input_tokens=2227, output_tokens=285
17:46:46,966 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:46:46,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.627420719712973. input_tokens=2156, output_tokens=237
17:46:55,888 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:46:55,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.542754461988807. input_tokens=3322, output_tokens=805
17:46:56,830 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:46:56,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 9.866642763838172. input_tokens=2208, output_tokens=207
17:46:58,553 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:46:58,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.297625590115786. input_tokens=3322, output_tokens=814
17:46:58,859 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:46:58,866 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.618724428117275. input_tokens=3322, output_tokens=826
17:47:00,665 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:00,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.400027006864548. input_tokens=3322, output_tokens=962
17:47:03,464 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:03,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 28.610478289425373. input_tokens=3323, output_tokens=957
17:47:03,863 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:03,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.617944613099098. input_tokens=2449, output_tokens=735
17:47:04,962 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:04,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 29.94407450966537. input_tokens=3322, output_tokens=872
17:47:05,274 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:05,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.03850091993809. input_tokens=3322, output_tokens=1038
17:47:07,35 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:07,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.703331634402275. input_tokens=2501, output_tokens=866
17:47:09,920 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:09,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.684307212010026. input_tokens=3323, output_tokens=907
17:47:10,165 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:10,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 35.06743286922574. input_tokens=3321, output_tokens=744
17:47:10,377 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:10,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.04973864927888. input_tokens=3322, output_tokens=1212
17:47:10,992 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:10,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.73704547248781. input_tokens=3322, output_tokens=1173
17:47:11,108 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:11,112 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.790691927075386. input_tokens=2369, output_tokens=898
17:47:12,398 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:12,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.09936875291169. input_tokens=2877, output_tokens=1199
17:47:13,902 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:13,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.62084866501391. input_tokens=34, output_tokens=150
17:47:16,111 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:16,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 40.89959145337343. input_tokens=3322, output_tokens=1111
17:47:17,318 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:17,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.05886000394821. input_tokens=2561, output_tokens=1241
17:47:18,503 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:18,509 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.20042834430933. input_tokens=3321, output_tokens=1271
17:47:18,560 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:18,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.563557451590896. input_tokens=34, output_tokens=192
17:47:19,304 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:19,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.436471842229366. input_tokens=34, output_tokens=344
17:47:19,429 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:19,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.464557839557528. input_tokens=34, output_tokens=374
17:47:19,482 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:19,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.016079617664218. input_tokens=34, output_tokens=537
17:47:21,592 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:21,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.30896580033004. input_tokens=3322, output_tokens=910
17:47:23,68 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:23,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 47.53585986793041. input_tokens=3256, output_tokens=995
17:47:24,162 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:24,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.762859087437391. input_tokens=34, output_tokens=337
17:47:26,738 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:26,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.070345563814044. input_tokens=2436, output_tokens=846
17:47:27,167 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:27,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.94642198458314. input_tokens=3322, output_tokens=1186
17:47:28,217 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:28,222 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.052296785637736. input_tokens=34, output_tokens=499
17:47:28,399 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:28,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 14.497704070061445. input_tokens=34, output_tokens=294
17:47:28,411 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:28,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.77106128260493. input_tokens=3308, output_tokens=1168
17:47:29,614 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:29,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 11.054064070805907. input_tokens=34, output_tokens=344
17:47:30,93 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:30,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.1754534766078. input_tokens=34, output_tokens=634
17:47:32,126 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:32,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.010572537779808. input_tokens=34, output_tokens=365
17:47:32,794 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:32,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.75797126069665. input_tokens=34, output_tokens=712
17:47:33,599 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:33,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 60.278228951618075. input_tokens=3321, output_tokens=1070
17:47:33,623 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:33,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.510682759806514. input_tokens=34, output_tokens=698
17:47:35,184 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:35,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.677397910505533. input_tokens=34, output_tokens=420
17:47:36,31 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:36,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 16.54641531035304. input_tokens=34, output_tokens=482
17:47:36,664 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:36,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.79482798837125. input_tokens=3323, output_tokens=1212
17:47:37,662 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:37,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.230611484497786. input_tokens=34, output_tokens=340
17:47:39,283 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:39,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 65.97295862808824. input_tokens=3322, output_tokens=1622
17:47:43,36 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:43,39 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.857398146763444. input_tokens=34, output_tokens=403
17:47:43,982 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:43,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.391223091632128. input_tokens=34, output_tokens=647
17:47:46,820 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:46,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.436796601861715. input_tokens=34, output_tokens=658
17:47:48,701 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:48,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.53880633600056. input_tokens=34, output_tokens=634
17:47:48,712 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:48,715 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.40031197294593. input_tokens=34, output_tokens=630
17:47:49,252 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:49,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.181124763563275. input_tokens=34, output_tokens=641
17:47:50,116 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:50,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.369428016245365. input_tokens=34, output_tokens=517
17:47:51,613 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:51,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.29221930541098. input_tokens=34, output_tokens=618
17:47:56,934 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:47:56,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 61.04070123285055. input_tokens=3322, output_tokens=1336
17:48:02,978 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:02,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.36745240353048. input_tokens=34, output_tokens=828
17:48:05,737 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:05,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 67.17095481976867. input_tokens=3322, output_tokens=1308
17:48:09,609 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:09,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.76721511036158. input_tokens=3322, output_tokens=2165
17:48:10,499 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:10,504 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.83085545711219. input_tokens=34, output_tokens=686
17:48:13,493 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:13,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.07392016425729. input_tokens=34, output_tokens=856
17:48:14,177 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:14,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 17.24089146964252. input_tokens=34, output_tokens=365
17:48:18,733 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:18,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.441188383847475. input_tokens=34, output_tokens=663
17:48:19,760 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:19,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.540473783388734. input_tokens=34, output_tokens=1238
17:48:21,559 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:21,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.817046450451016. input_tokens=34, output_tokens=344
17:48:28,878 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:28,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.26301048323512. input_tokens=34, output_tokens=414
17:48:29,208 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:48:29,210 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:48:29,211 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:48:29,213 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:48:29,215 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:48:29,216 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:48:29,222 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:48:29,222 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:48:29,223 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:48:29,224 graphrag.llm.base.rate_limiting_llm WARNING summarize failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:48:31,288 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:31,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.3233254868537188. input_tokens=322, output_tokens=54
17:48:32,639 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:32,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.594331754371524. input_tokens=325, output_tokens=87
17:48:33,310 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:33,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.263217177242041. input_tokens=360, output_tokens=135
17:48:33,447 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:33,451 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.463605090975761. input_tokens=321, output_tokens=102
17:48:33,567 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:33,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.507539169862866. input_tokens=409, output_tokens=169
17:48:34,28 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:34,31 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.016464436426759. input_tokens=398, output_tokens=175
17:48:34,382 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:34,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.319347096607089. input_tokens=305, output_tokens=80
17:48:34,424 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:34,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.386707426980138. input_tokens=316, output_tokens=102
17:48:34,531 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:34,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.465737445279956. input_tokens=391, output_tokens=133
17:48:34,908 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:34,910 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.617063343524933. input_tokens=359, output_tokens=142
17:48:35,261 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:35,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.206172678619623. input_tokens=377, output_tokens=191
17:48:35,460 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:35,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.459220627322793. input_tokens=332, output_tokens=165
17:48:35,599 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:35,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.576434610411525. input_tokens=352, output_tokens=168
17:48:36,13 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:36,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.9633981473743916. input_tokens=340, output_tokens=173
17:48:36,256 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:36,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 5.462802052497864. input_tokens=348, output_tokens=198
17:48:36,451 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:36,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.4216716401278973. input_tokens=342, output_tokens=70
17:48:36,684 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:36,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.614804446697235. input_tokens=526, output_tokens=254
17:48:37,225 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:37,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.207209080457687. input_tokens=496, output_tokens=178
17:48:37,389 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:37,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.348122544586658. input_tokens=349, output_tokens=169
17:48:37,569 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:37,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 6.662725469097495. input_tokens=389, output_tokens=246
17:48:37,586 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:37,593 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.135809911414981. input_tokens=443, output_tokens=118
17:48:37,845 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:37,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.848754622042179. input_tokens=434, output_tokens=255
17:48:38,394 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:38,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 7.40987216681242. input_tokens=511, output_tokens=240
17:48:38,771 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:38,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.860824352130294. input_tokens=327, output_tokens=98
17:48:38,905 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:38,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.590046059340239. input_tokens=364, output_tokens=194
17:48:38,963 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:38,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.36002891510725. input_tokens=333, output_tokens=86
17:48:39,463 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:39,466 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.078716339543462. input_tokens=419, output_tokens=158
17:48:39,499 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:39,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.481171188876033. input_tokens=345, output_tokens=77
17:48:39,615 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:39,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.351139221340418. input_tokens=345, output_tokens=133
17:48:40,34 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.464527748525143. input_tokens=392, output_tokens=204
17:48:40,237 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.595384432002902. input_tokens=364, output_tokens=184
17:48:40,469 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.01524599455297. input_tokens=369, output_tokens=128
17:48:40,486 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.06090535223484. input_tokens=351, output_tokens=145
17:48:40,531 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.960925241932273. input_tokens=327, output_tokens=95
17:48:41,589 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7405500449240208. input_tokens=361, output_tokens=140
17:48:41,794 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,797 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.259855329990387. input_tokens=301, output_tokens=123
17:48:41,824 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.135002724826336. input_tokens=357, output_tokens=141
17:48:42,12 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.754769258201122. input_tokens=397, output_tokens=201
17:48:43,45 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.5863070003688335. input_tokens=387, output_tokens=298
17:48:43,300 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.831569703295827. input_tokens=371, output_tokens=92
17:48:43,366 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.967529010027647. input_tokens=355, output_tokens=152
17:48:43,485 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9526592325419188. input_tokens=298, output_tokens=81
17:48:43,705 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.098139118403196. input_tokens=351, output_tokens=124
17:48:43,816 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.9115937538445. input_tokens=349, output_tokens=140
17:48:44,198 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.23499889485538. input_tokens=313, output_tokens=136
17:48:44,311 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.719252297654748. input_tokens=327, output_tokens=94
17:48:44,333 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,335 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.832495113834739. input_tokens=312, output_tokens=109
17:48:44,430 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.965781357139349. input_tokens=313, output_tokens=110
17:48:44,466 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.070016600191593. input_tokens=383, output_tokens=132
17:48:44,913 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1187810115516186. input_tokens=352, output_tokens=104
17:48:44,972 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.380090897902846. input_tokens=363, output_tokens=186
17:48:45,170 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.142138907685876. input_tokens=2068, output_tokens=477
17:48:45,226 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 14.592761682346463. input_tokens=817, output_tokens=365
17:48:45,374 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,377 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.6020381059497595. input_tokens=396, output_tokens=165
17:48:45,496 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.26997047662735. input_tokens=395, output_tokens=246
17:48:45,694 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.457789529114962. input_tokens=400, output_tokens=154
17:48:46,375 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.399349393323064. input_tokens=949, output_tokens=465
17:48:46,706 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.992489542812109. input_tokens=350, output_tokens=119
17:48:46,954 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6206424944102764. input_tokens=356, output_tokens=83
17:48:46,965 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 1 retries took 15.986741296947002. input_tokens=761, output_tokens=295
17:48:46,978 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1601300686597824. input_tokens=343, output_tokens=91
17:48:47,218 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.017928333953023. input_tokens=342, output_tokens=104
17:48:47,243 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 18.248181007802486. input_tokens=1358, output_tokens=457
17:48:47,725 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.708983963355422. input_tokens=375, output_tokens=187
17:48:47,783 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.958316680043936. input_tokens=343, output_tokens=100
17:48:48,183 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,186 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.815831819549203. input_tokens=345, output_tokens=87
17:48:48,320 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.831839714199305. input_tokens=340, output_tokens=102
17:48:49,456 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.282922774553299. input_tokens=370, output_tokens=173
17:48:49,608 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,610 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.572452848777175. input_tokens=442, output_tokens=305
17:48:49,681 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.709480011835694. input_tokens=415, output_tokens=180
17:48:49,819 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.354467339813709. input_tokens=318, output_tokens=127
17:48:49,834 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.1231160312891006. input_tokens=346, output_tokens=112
17:48:49,866 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.636473132297397. input_tokens=362, output_tokens=119
17:48:50,297 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.810713943094015. input_tokens=333, output_tokens=124
17:48:50,671 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.620051430538297. input_tokens=358, output_tokens=216
17:48:50,748 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,751 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.43725461140275. input_tokens=463, output_tokens=276
17:48:50,940 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6980102192610502. input_tokens=315, output_tokens=132
17:48:51,167 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.3848584182560444. input_tokens=354, output_tokens=121
17:48:51,210 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.24473668076098. input_tokens=343, output_tokens=131
17:48:51,343 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,351 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.369474153965712. input_tokens=355, output_tokens=191
17:48:51,375 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,377 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.069762714207172. input_tokens=416, output_tokens=299
17:48:51,519 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.023564480245113. input_tokens=448, output_tokens=198
17:48:51,768 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4498257357627153. input_tokens=356, output_tokens=133
17:48:51,785 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.406656628474593. input_tokens=330, output_tokens=166
17:48:51,943 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7611571196466684. input_tokens=356, output_tokens=106
17:48:52,263 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.349864261224866. input_tokens=335, output_tokens=155
17:48:52,380 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.5446786396205425. input_tokens=351, output_tokens=56
17:48:52,517 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.299191577360034. input_tokens=379, output_tokens=153
17:48:52,604 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.227154329419136. input_tokens=420, output_tokens=184
17:48:53,328 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,332 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.602996252477169. input_tokens=353, output_tokens=157
17:48:53,645 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.974938839673996. input_tokens=346, output_tokens=99
17:48:53,722 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8623578026890755. input_tokens=317, output_tokens=100
17:48:54,14 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.557360576465726. input_tokens=342, output_tokens=146
17:48:54,166 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.557125115767121. input_tokens=346, output_tokens=93
17:48:54,334 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.040196416899562. input_tokens=326, output_tokens=111
17:48:54,538 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.723356179893017. input_tokens=350, output_tokens=140
17:48:54,758 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,761 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.066551469266415. input_tokens=365, output_tokens=121
17:48:55,221 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:55,224 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.525155512616038. input_tokens=333, output_tokens=154
17:48:55,861 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:48:55,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.91142007522285. input_tokens=452, output_tokens=244
17:48:59,994 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:49:00,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.571400634944439. input_tokens=767, output_tokens=386
17:49:00,535 graphrag.index.run.workflow INFO dependencies for create_final_covariates: ['create_base_text_units']
17:49:00,536 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:00,545 datashaper.workflow.workflow INFO executing verb create_final_covariates
17:49:00,708 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_covariates.parquet
17:49:01,81 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:49:01,82 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:01,93 datashaper.workflow.workflow INFO executing verb create_final_entities
17:49:01,185 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:49:01,552 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:49:01,553 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:01,565 datashaper.workflow.workflow INFO executing verb create_final_nodes
17:49:01,907 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:49:02,296 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:49:02,298 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:02,312 datashaper.workflow.workflow INFO executing verb create_final_communities
17:49:02,543 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:49:02,960 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
17:49:02,961 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:02,962 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:49:03,57 datashaper.workflow.workflow INFO executing verb create_final_relationships
17:49:03,192 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:49:03,556 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_covariates', 'create_final_relationships', 'create_base_text_units']
17:49:03,575 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:49:03,592 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:49:03,604 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:49:03,613 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:03,638 datashaper.workflow.workflow INFO executing verb create_final_text_units
17:49:03,716 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:49:04,97 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_covariates', 'create_final_nodes', 'create_final_relationships', 'create_final_communities']
17:49:04,98 graphrag.utils.storage INFO read table from storage: create_final_covariates.parquet
17:49:04,110 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:49:04,122 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:49:04,132 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:49:04,171 datashaper.workflow.workflow INFO executing verb create_final_community_reports
17:49:04,205 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=2 => 44
17:49:04,306 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 158
17:49:04,559 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 213
17:49:40,280 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:49:40,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.501356560736895. input_tokens=2898, output_tokens=1133
17:49:48,935 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:49:48,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 44.166093135252595. input_tokens=2095, output_tokens=1100
17:49:57,772 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:49:57,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 53.013858603313565. input_tokens=2886, output_tokens=1167
17:50:17,934 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:17,941 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 73.18168669752777. input_tokens=7640, output_tokens=1434
17:50:18,32 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:18,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 73.27525629289448. input_tokens=4996, output_tokens=1321
17:50:18,474 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:50:18,476 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:50:18,477 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:50:18,479 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:50:18,480 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:50:18,482 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:50:18,484 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:50:18,486 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:50:18,487 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
17:50:18,488 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
17:50:45,165 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:45,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.999912101775408. input_tokens=2025, output_tokens=907
17:50:48,78 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:48,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.793936278671026. input_tokens=2792, output_tokens=945
17:50:53,299 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:53,309 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 33.57080540806055. input_tokens=2082, output_tokens=1082
17:50:53,952 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:53,957 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 35.644588340073824. input_tokens=2335, output_tokens=943
17:50:54,544 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:54,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.326186388731. input_tokens=2399, output_tokens=1029
17:50:55,84 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:55,91 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.8774474170059. input_tokens=1847, output_tokens=1087
17:50:55,407 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:55,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.09306509420276. input_tokens=2366, output_tokens=1072
17:50:56,62 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:56,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.849113296717405. input_tokens=2044, output_tokens=992
17:50:56,75 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:56,85 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.801993602886796. input_tokens=2173, output_tokens=1062
17:50:56,451 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:56,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.250604793429375. input_tokens=2544, output_tokens=1284
17:50:57,462 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:57,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 37.61115098372102. input_tokens=2722, output_tokens=1077
17:50:58,573 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:58,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 38.223483162000775. input_tokens=1770, output_tokens=925
17:50:59,308 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:50:59,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.007604790851474. input_tokens=2793, output_tokens=863
17:51:00,437 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:00,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.18215946853161. input_tokens=2248, output_tokens=1071
17:51:03,736 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:03,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 43.86172265186906. input_tokens=2446, output_tokens=1162
17:51:04,283 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:04,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.039845338091254. input_tokens=2090, output_tokens=1038
17:51:04,940 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:04,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.67984789982438. input_tokens=2307, output_tokens=1103
17:51:05,477 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:05,484 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 47.15781581029296. input_tokens=3346, output_tokens=1319
17:51:05,942 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:05,946 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 47.75173660367727. input_tokens=6408, output_tokens=1093
17:51:06,689 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:06,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.50806928239763. input_tokens=3005, output_tokens=958
17:51:11,201 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:11,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 52.932909520342946. input_tokens=2455, output_tokens=1213
17:51:15,867 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:15,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 57.69669270887971. input_tokens=7460, output_tokens=1251
17:51:16,825 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:16,833 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 58.63061816431582. input_tokens=3785, output_tokens=1283
17:51:27,669 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:27,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 68.1227401457727. input_tokens=3701, output_tokens=1393
17:51:29,622 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:29,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 44.45155720226467. input_tokens=4679, output_tokens=1371
17:51:30,296 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:30,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 72.03147839009762. input_tokens=3834, output_tokens=1292
17:51:57,894 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:51:57,899 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:51:57,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.416148703545332. input_tokens=1885, output_tokens=916
17:52:09,609 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:09,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.06676254607737. input_tokens=2785, output_tokens=1349
17:52:11,54 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:11,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.57098828442395. input_tokens=1831, output_tokens=890
17:52:11,683 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:11,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.14727853983641. input_tokens=5732, output_tokens=1223
17:52:13,674 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:13,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 43.15119046904147. input_tokens=4604, output_tokens=1142
17:52:15,839 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:15,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.293677454814315. input_tokens=2142, output_tokens=961
17:52:25,78 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:25,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 54.585342122241855. input_tokens=2956, output_tokens=1163
17:52:27,436 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:27,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 56.928297558799386. input_tokens=5638, output_tokens=989
17:52:39,693 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:39,697 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:52:39,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 69.21896925754845. input_tokens=9358, output_tokens=1337
17:52:42,305 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:42,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 71.7846448700875. input_tokens=2410, output_tokens=954
17:52:42,402 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:42,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 71.89917466044426. input_tokens=5438, output_tokens=1370
17:52:49,49 httpx INFO HTTP Request: POST http://localhost:13000/v1/chat/completions "HTTP/1.1 200 OK"
17:52:49,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 78.53776389732957. input_tokens=5389, output_tokens=1354
17:52:49,82 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:52:49,589 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_final_text_units']
17:52:49,590 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:52:49,620 datashaper.workflow.workflow INFO executing verb create_final_documents
17:52:49,639 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:52:50,86 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_text_units', 'create_final_entities', 'create_final_relationships', 'create_final_community_reports']
17:52:50,87 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:52:50,96 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:52:50,108 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:52:50,124 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:52:50,136 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:52:50,195 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
17:52:50,203 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:52:50,203 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:52:50,205 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://localhost:13000/v1
17:52:50,272 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-v1: TPM=0, RPM=0
17:52:50,272 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-v1: 25
17:52:50,305 graphrag.index.operations.embed_text.strategies.openai INFO embedding 32 inputs via 32 snippets using 4 batches. max_batch_size=16, max_tokens=8191
17:52:50,661 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:50,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.45713433995842934. input_tokens=7971, output_tokens=0
17:52:50,782 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:50,783 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:50,784 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:50,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5577191170305014. input_tokens=8062, output_tokens=0
17:52:50,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.636277362704277. input_tokens=6392, output_tokens=0
17:52:51,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7458436284214258. input_tokens=7805, output_tokens=0
17:52:51,210 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:52:51,243 graphrag.index.operations.embed_text.strategies.openai INFO embedding 213 inputs via 213 snippets using 14 batches. max_batch_size=16, max_tokens=8191
17:52:51,505 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,578 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.30379778891801834. input_tokens=185, output_tokens=0
17:52:51,588 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,785 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.5273236259818077. input_tokens=947, output_tokens=0
17:52:51,810 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,811 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,811 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,812 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,813 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,813 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,814 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,815 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,815 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,816 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,816 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:51,817 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:52,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7934042904525995. input_tokens=1869, output_tokens=0
17:52:52,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0159080289304256. input_tokens=1212, output_tokens=0
17:52:52,505 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2568510565906763. input_tokens=1200, output_tokens=0
17:52:52,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.472360260784626. input_tokens=2448, output_tokens=0
17:52:52,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6628637071698904. input_tokens=1471, output_tokens=0
17:52:53,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.842438779771328. input_tokens=1445, output_tokens=0
17:52:53,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.087782233953476. input_tokens=1211, output_tokens=0
17:52:53,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3132420014590025. input_tokens=1748, output_tokens=0
17:52:53,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.4938340932130814. input_tokens=1157, output_tokens=0
17:52:53,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.7012534867972136. input_tokens=1015, output_tokens=0
17:52:54,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.928138015791774. input_tokens=1063, output_tokens=0
17:52:54,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 3.189437700435519. input_tokens=1342, output_tokens=0
17:52:54,649 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:52:54,722 graphrag.index.operations.embed_text.strategies.openai INFO embedding 43 inputs via 43 snippets using 6 batches. max_batch_size=16, max_tokens=8191
17:52:54,948 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:54,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.2138234656304121. input_tokens=1037, output_tokens=0
17:52:55,60 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:55,61 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:55,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.43088966235518456. input_tokens=7952, output_tokens=0
17:52:55,174 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:55,175 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:55,176 httpx INFO HTTP Request: POST http://localhost:13000/v1/embeddings "HTTP/1.1 200 OK"
17:52:55,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.530897980555892. input_tokens=7605, output_tokens=0
17:52:55,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6706711370497942. input_tokens=7567, output_tokens=0
17:52:55,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8151056617498398. input_tokens=7953, output_tokens=0
17:52:55,665 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9068991616368294. input_tokens=8166, output_tokens=0
17:52:55,802 graphrag.cli.index INFO All workflows completed successfully.
